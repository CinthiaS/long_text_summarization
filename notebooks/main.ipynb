{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('estudos': conda)"
  },
  "interpreter": {
   "hash": "8189520348208f2d7a55cd5e08e528a8ea0eb6334cdd1bc6eb49a516298cd84a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import *\n",
    "from summarizer import Summarizer\n",
    "from summarizer.coreference_handler import CoreferenceHandler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import os\n",
    "from xml.etree import ElementTree\n",
    "from xml.dom import minidom\n",
    "from xml.etree.ElementTree import Element, SubElement, Comment\n",
    "import numpy as np\n",
    "\n",
    "from sumeval.metrics.rouge import RougeCalculator\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "def eval(\n",
    "    reference_summary, model_summary, metrics=[\"ROUGE_1\", \"ROUGE_2\", \"ROUGE_L\", \"NUBIA\", \"BLEURT\"]):\n",
    "\n",
    "    rouge = RougeCalculator(stopwords=True, lang=\"en\")\n",
    "\n",
    "    if(\"ROUGE_1\" in metrics):\n",
    "      rouge_1 = rouge.rouge_n( summary=model_summary, references=reference_summary, n=1)\n",
    "\n",
    "    else:\n",
    "      rouge_1 = None\n",
    "\n",
    "    if(\"ROUGE_2\" in metrics):\n",
    "      rouge_2 = rouge.rouge_n(summary=model_summary,references=[reference_summary],n=2)\n",
    "    else:\n",
    "      rouge_2 = None\n",
    "\n",
    "    if(\"ROUGE_L\" in metrics):\n",
    "      rouge_l = rouge.rouge_l( summary=model_summary,references=[reference_summary])\n",
    "    else:\n",
    "      rouge_l = None\n",
    "      \n",
    "    if(\"BLEU\" in metrics):\n",
    "      try:\n",
    "          bleu = 0#sentence_bleu( reference_summary.split(\" \"), model_summary.split(\" \"), smoothing_function=smoothie)\n",
    "      except:\n",
    "          bleu = 0\n",
    "    else:\n",
    "      bleu = None\n",
    "    return rouge_1, rouge_2,rouge_l, bleu\n",
    "\n",
    "def prettify(elem):\n",
    "      \"\"\"Return a pretty-printed XML string for the Element.\n",
    "      \"\"\"\n",
    "      rough_string = ElementTree.tostring(elem, 'utf-8')\n",
    "      reparsed = minidom.parseString(rough_string)\n",
    "      return reparsed.toprettyxml(indent=\"  \")\n",
    "  \n",
    "def create_report_valid(\n",
    "    summary_array, references_summary, article, name_file,\n",
    "     metrics=[\"ROUGE_1\", \"ROUGE_2\", \"ROUGE_L\", \"NUBIA\", \"BLEURT\"]):\n",
    "\n",
    "  rouge_1_arr  = []\n",
    "  rouge_2_arr  = []\n",
    "  rouge_L_arr  = []\n",
    "  bleu_arr = []\n",
    "\n",
    "  top = Element('ZakSum')\n",
    "\n",
    "  comment = Comment('Generated by Amr Zaki')\n",
    "  top.append(comment)\n",
    "\n",
    "  i=0\n",
    "  for summ in summary_array:\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "      \n",
    "    example = SubElement(top, 'example')\n",
    "    article_element   = SubElement(example, 'article')\n",
    "    article_element.text = article[i]\n",
    "  \n",
    "    reference_element = SubElement(example, 'reference')\n",
    "    reference_element.text = references_summary[i]\n",
    "  \n",
    "    summary_element   = SubElement(example, 'summary')\n",
    "    summary_element.text = summ\n",
    "\n",
    "    if(len(summ) != 0):\n",
    "      rouge_1, rouge_2, rouge_L, bleu = eval(references_summary[i],summ, metrics=metrics )\n",
    "    else: \n",
    "      rouge_1 = rouge_2 = rouge_L = 0\n",
    "\n",
    "    if(rouge_1 != None):\n",
    "      rouge_1_arr.append(rouge_1) \n",
    "    if(rouge_2 != None):\n",
    "      rouge_2_arr.append(rouge_2)\n",
    "    if(rouge_L != None):\n",
    "      rouge_L_arr.append(rouge_L)\n",
    "    if(bleu != None):\n",
    "      bleu_arr.append(bleu)\n",
    "\n",
    "    i+=1\n",
    "\n",
    "    eval_element = SubElement(example, 'eval')\n",
    "    ROUGE_1_element  = SubElement(eval_element, 'ROUGE_1' , {'score':str(rouge_1)})\n",
    "    ROUGE_2_element  = SubElement(eval_element, 'ROUGE_2' , {'score':str(rouge_2)})\n",
    "    ROUGE_L_element  = SubElement(eval_element, 'ROUGE_l' , {'score':str(rouge_L)})\n",
    "    BLEU_element  = SubElement(eval_element, 'BLEU' , {'score':str(bleu_arr)})\n",
    "\n",
    "  if(rouge_1_arr != []): top.set('rouge_1', str(np.mean(rouge_1_arr)))\n",
    "  if(rouge_2_arr != []): top.set('rouge_2', str(np.mean(rouge_2_arr)))\n",
    "  if(rouge_L_arr != []): top.set('rouge_L', str(np.mean(rouge_L_arr)))\n",
    "  if(bleu_arr != []): top.set('BLEU', str(np.mean(bleu_arr)))\n",
    "\n",
    "\n",
    "  with open(name_file, \"w+\") as f:\n",
    "    print(prettify(top), file=f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from sumy.summarizers.lex_rank import LexRankSummarizer as SummarizerLex\n",
    "from sumy.summarizers.sum_basic import SumBasicSummarizer as SummarizerSumBasic\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer  as SummarizerTextrank\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "\n",
    "def summarization_one_file(summarizer, parser, SENTENCES_COUNT):\n",
    "\n",
    "    sentences = []\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        sentences.append(str(sentence))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def summarization_all_files(df, model='lex', section='intro', SENTENCES_COUNT=3):\n",
    "\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    if model == 'lex':\n",
    "\n",
    "        summarizer = SummarizerLex(stemmer)\n",
    "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    \n",
    "    elif model == 'textrank':\n",
    "\n",
    "        summarizer = SummarizerTextrank(stemmer)\n",
    "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "    elif model == 'sumbasic':\n",
    "\n",
    "        summarizer = SummarizerSumBasic(stemmer)\n",
    "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "\n",
    "    summaries = []\n",
    "\n",
    "    f = open(\"{}_{}_summ.txt\".format(model, section), 'w')\n",
    "    for text in df['pp_reference']:\n",
    "\n",
    "        parser = PlaintextParser(text, Tokenizer(LANGUAGE))\n",
    "        summ = summarization_one_file(summarizer, parser, SENTENCES_COUNT=3)\n",
    "        summ = ' '.join(summ)\n",
    "        summaries.append(summ)\n",
    "        f.write(summ)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return summaries"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def summarization_all_files_bert(df, model, model_name='bert_basic', section='intro', SENTENCES_COUNT=3):\n",
    "\n",
    "    summaries = []\n",
    "\n",
    "    f = open(\"{}_{}_summ.txt\".format(model_name, section), 'w')\n",
    "    for text in df['pp_reference']:\n",
    "\n",
    "        summ = model(text, num_sentences=SENTENCES_COUNT)\n",
    "        summaries.append(summ)\n",
    "        f.write(summ)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return summaries"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def load_name_files(path_base, files):\n",
    "\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        texts.append(json.load(open('{}/{}'.format(path_base, file))))\n",
    "\n",
    "    return texts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def load_files(files, path_base):\n",
    "\n",
    "    section_1 = []\n",
    "    section_2 = []\n",
    "    section_3 = []\n",
    "    section_4 = []\n",
    "    keywords = []\n",
    "\n",
    "    texts = load_name_files(path_base, files)\n",
    "\n",
    "    for i in texts:\n",
    "\n",
    "        section_1.append(format_intro(i.get('sec_abstract')))\n",
    "        section_2.append(format_intro(i.get('sec_introduction')))\n",
    "        section_3.append(format_intro(i.get('sec_materials_and_methods')))\n",
    "        section_4.append(format_intro(i.get('sec_results_and_conclusion')))\n",
    "        keywords.append(i.get('sec_keyword'))\n",
    "\n",
    "    return section_1, section_2, section_3, section_4, keywords"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def get_number_sentences(text):\n",
    "\n",
    "    model = Summarizer()\n",
    "    k = model.calculate_optimal_k(text, k_max=5)\n",
    "\n",
    "    return k"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def get_citations(text):\n",
    "\n",
    "  soup = BeautifulSoup(text, 'html.parser')\n",
    "  bib = soup.findAll('xref')\n",
    "\n",
    "  return bib\n",
    "\n",
    "def replace_bib(text, bibs):\n",
    "\n",
    "  for i in bibs:\n",
    "    text = text.replace(str(i), '')\n",
    "    \n",
    "  return text\n",
    "\n",
    "def remove_citations(xml, text):\n",
    "  \n",
    "  bibs = get_citations(xml)\n",
    "  text = replace_bib(text, bibs)\n",
    "  text = format_text(text, post_processing=True)\n",
    "  \n",
    "  return text\n",
    "\n",
    "def format_intro(text):\n",
    "\n",
    "  text = text.replace(\"INTRODUCTION\", \"\")\n",
    "  text = text.replace(\"Introduction\", \"\")\n",
    "  text = text.replace('\\n\\nOBJECTIVE\\n', '')\n",
    "  text = text.replace('\\n\\nObjectives\\n', '')\n",
    "  text = text.replace('\\nSummary\\n\\n', '')\n",
    "  text = text.replace(\"\\n\", \"\")\n",
    "\n",
    "  return text\n",
    "\n",
    "def format_xml(xml):\n",
    "\n",
    "  xml = xml.replace(\".<xref\", \". <xref\")\n",
    "  xml = xml.replace(\"</p>\",\"</p>  \" )\n",
    "  xml = xml.replace('.</p>', \"</p>.\")\n",
    "  xml = xml.replace('<title-introduction><title></title>', '')\n",
    "  xml = xml.replace('</title-introduction>', '')\n",
    "  xml = xml.replace(\"<italic>et al</italic>.\", \"<italic>et al</italic>\")\n",
    "\n",
    "  return xml\n",
    "\n",
    "def format_text(text, post_processing=False):\n",
    "\n",
    "  text = text.replace(\".<xref\", \". <xref\")\n",
    "  text = text.replace(\"</p>\",\"</p> \")\n",
    "  text = text.replace('.</p>', \"</p>.\")\n",
    "  if post_processing:\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    text = text.replace(\"–\", '')\n",
    "    text = re.sub(r'(?s)\\(.*?\\)', '', text) \n",
    "    text = re.sub(r'(?s)\\[.*?\\]', '', text) \n",
    "    text = text.replace(\"(,)\", \"\")\n",
    "    text = text.replace(\"()\", \"\")\n",
    "    text = text.replace(\"[,]\", \"\")\n",
    "    text = text.replace(\"[]\", \"\")\n",
    "    text = text.replace(\"(; )\", \"\")\n",
    "    text = text.replace(\"(; )\", \"\")\n",
    "    text = re.sub(r'(?s)<title>.*?</title>', '', text) \n",
    "\n",
    "  return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def preprocess(section, reference):\n",
    "\n",
    "    xml = format_xml(str(section))\n",
    "    text = format_text(str(section), post_processing=False)\n",
    "    reference = format_text(str(reference), post_processing=True)\n",
    "\n",
    "    bibs = get_citations(xml)\n",
    "    text = replace_bib(text, bibs)\n",
    "    text = format_text(text, post_processing=True)\n",
    "\n",
    "    soup = BeautifulSoup(text)\n",
    "    text = soup.get_text()\n",
    "\n",
    "    soup = BeautifulSoup(reference)\n",
    "    reference = soup.get_text()\n",
    "\n",
    "    return text, reference"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def preprocess_all(sources, references):\n",
    "\n",
    "    pp_source = []\n",
    "    pp_references = []\n",
    "\n",
    "    for i in range(len(references)):\n",
    "\n",
    "        text, reference = preprocess(sources[i], references[i])\n",
    "\n",
    "        pp_source.append(text)\n",
    "        pp_references.append(text)\n",
    "\n",
    "    pp_texts = {'pp_source': pp_source, 'pp_reference': pp_references}\n",
    "\n",
    "    return pp_texts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def evaluation(candidates, references, sources, algorithm, section):\n",
    "\n",
    "    metrics=[\"ROUGE_1\", \"ROUGE_2\", \"ROUGE_L\", \"BLEU\"]\n",
    "    create_report_valid(\n",
    "            candidates, references, sources,\n",
    "            name_file=\"../validation/validation_{}_{}.xml\".format(algorithm, section),\n",
    "            metrics=metrics)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import numpy as np\n",
    "\n",
    "def count_len(text):\n",
    "\n",
    "    count_sentences = []\n",
    "    count_words = []\n",
    "\n",
    "    for i in text:\n",
    "        count_sentences.append(len(i.split('.')))\n",
    "        count_words.append(len(i.split(' ')))\n",
    "\n",
    "    print(\"Número médio de sentenças: {}\".format(np.mean(count_sentences)))\n",
    "    print(\"Número médio de palavras: {}\".format(np.mean(count_words)))\n",
    "\n",
    "    return count_sentences, count_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "df = pd.read_csv(\"files.csv\")\n",
    "name_files = df['files'].tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "path_base = '../../sumdata/dataset_articles'\n",
    "section_1, section_2, section_3, section_4, keywords = load_files(name_files, path_base)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-37486f271020>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../sumdata/dataset_articles'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msection_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msection_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msection_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msection_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-1368d26f915a>\u001b[0m in \u001b[0;36mload_files\u001b[0;34m(files, path_base)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_name_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-fe122c0660c4>\u001b[0m in \u001b[0;36mload_name_files\u001b[0;34m(path_base, files)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/estudos/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/estudos/lib/python3.8/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "all_section = [section_2[i] + \" \" +  section_3[i] + \" \" + section_4[i] for i in range(len(section_1))]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "LANGUAGE = \"english\"\n",
    "\n",
    "k2 = [3] *len(section_1)\n",
    "k3 = [3] *len(section_1)\n",
    "k4 = [3] *len(section_1)\n",
    "k5 = [9] *len(section_1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "pp_intro = preprocess_all(section_2, section_1)\n",
    "pp_mat = preprocess_all(section_3, section_1)\n",
    "pp_conc = preprocess_all(section_4, section_1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "pp_all = preprocess_all(all_section, section_1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "f = open(\"bert_custom_all.txt\").readlines()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "print(\"Evaluation TextRank Results\")\n",
    "evaluation(candidates=f, references=pp_all['pp_reference'], sources=pp_all['pp_source'], algorithm=\"bert-custom\", section=\"allll\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluation TextRank Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "custom_config = AutoConfig.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "custom_config.output_hidden_states=True\n",
    "custom_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "custom_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', config=custom_config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bertbasic = Summarizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "count_sentences, count_words = count_len(pp_intro['pp_reference'])\n",
    "values['sentences_ref'] = count_sentences\n",
    "values['words_ref'] = count_words"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Número médio de sentenças: 10.873\n",
      "Número médio de palavras: 210.2152\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "count_sentences, count_words = count_len(pp_intro['pp_source'])\n",
    "values['sentences_intro'] = count_sentences\n",
    "values['words_intro'] = count_words"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Número médio de sentenças: 22.6116\n",
      "Número médio de palavras: 540.2956\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LexRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "candidates_lex = summarization_all_files(pp_intro, model='lex', section='intro', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TextRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_text = summarization_all_files(pp_intro, model='texrank', section='intro', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SumBasic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_sumbasic = summarization_all_files(pp_intro, model='sumbasic', section='intro', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BERT-Basic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_bertbasic = summarization_all_files_bert(pp_intro, bertbasic, model_name='bert_basic', section='intro', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SciBERT Summ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "candidates_custombert = summarization_all_files_bert(pp_intro, custom_model, model_name='custom_bert', section='intro', SENTENCES_COUNT=3)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-eb1b9ec2fb2c>, line 1)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-eb1b9ec2fb2c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    SciBERT Summ\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "print(\"Evaluation TextRank Results\")\n",
    "evaluation(candidates=candidates_text, references=pp_intro['pp_reference'], sources=pp_intro['pp_source'], algorithm=\"text\", section=\"intro\")\n",
    "print(\"\\nEvaluation LexRank Results\")\n",
    "evaluation(candidates=candidates_lex, references=pp_intro['pp_reference'], sources=pp_intro['pp_source'], algorithm=\"lex\", section=\"intro\")\n",
    "print(\"\\nEvaluation Sumbasic Results\")\n",
    "evaluation(candidates=candidates_sumbasic, references=pp_intro['pp_reference'], sources=pp_intro['pp_source'], algorithm=\"sumbasic\", section=\"intro\")\n",
    "print(\"\\nEvaluation Sumbasic Results\")\n",
    "evaluation(candidates=candidates_bertbasic, references=pp_intro['pp_reference'], sources=pp_intro['pp_source'], algorithm=\"bertbasic\", section=\"intro\")\n",
    "print(\"\\nEvaluation SciBERT Summ Results\")\n",
    "evaluation(candidates=candidates_custombert, references=pp_intro['pp_reference'], sources=pp_intro['pp_source'], algorithm=\"custombert\", section=\"intro\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Materials and Methods"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "count_sentences, count_words = count_len(source)\n",
    "values['sentences_mat'] = count_sentences\n",
    "values['words_mat'] = count_words"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Número médio de sentenças: 59.1334\n",
      "Número médio de palavras: 1077.3682\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LexRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "candidates_lex = summarization_all_files(pp_mat, model='lex', section='mat', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TextRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_text = summarization_all_files(pp_mat, model='texrank', section='mat', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sumbasic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_sumbasic = summarization_all_files(pp_mat, model='sumbasic', section='mat', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BERT Basic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_bertbasic = summarization_all_files_bert(pp_mat, bertbasic, model_name='bert_basic', section='mat', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scibert summ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_custombert = summarization_all_files_bert(pp_mat, custom_model, model_name='custom_bert', section='mat', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "print(\"Evaluation TextRank Results\")\n",
    "evaluation(candidates=candidates_text, references=pp_mat['pp_reference'], sources=pp_mat['pp_source'], algorithm=\"text\", section=\"mat\")\n",
    "print(\"\\nEvaluation LexRank Results\")\n",
    "evaluation(candidates=candidates_lex, references=pp_mat['pp_reference'], sources=pp_mat['pp_source'], algorithm=\"lex\", section=\"mat\")\n",
    "print(\"\\nEvaluation Sumbasic Results\")\n",
    "evaluation(candidates=candidates_sumbasic, references=pp_mat['pp_reference'], sources=pp_mat['pp_source'], algorithm=\"sumbasic\", section=\"mat\")\n",
    "print(\"\\nEvaluation Sumbasic Results\")\n",
    "evaluation(candidates=candidates_bertbasic, references=pp_mat['pp_reference'], sources=pp_mat['pp_source'], algorithm=\"bertbasic\", section=\"mat\")\n",
    "print(\"\\nEvaluation SciBERT Summ Results\")\n",
    "evaluation(candidates=candidates_custombert, references=pp_mat['pp_reference'], sources=pp_mat['pp_source'], algorithm=\"custombert\", section=\"mat\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Evaluation TextRank Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "\n",
      "Evaluation LexRank Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "\n",
      "Evaluation Sumbasic Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "count_sentences, count_words = count_len(source)\n",
    "values['sentences_conc'] = count_sentences\n",
    "values['words_conc'] = count_words"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Número médio de sentenças: 109.9564\n",
      "Número médio de palavras: 2081.2918\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LexRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "candidates_lex = summarization_all_files(pp_conc, model='lex', section='conc', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TextRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_text = summarization_all_files(pp_conc, model='texrank', section='conc', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SumBasic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_sumbasic = summarization_all_files(pp_conc, model='sumbasic', section='conc', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BERT - Basic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_bertbasic = summarization_all_files_bert(pp_conc, bertbasic, model_name='bert_basic', section='conc', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scibert Summ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_custombert = summarization_all_files_bert(pp_conc, custom_model, model_name='custom_bert', section='conc', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "print(\"Evaluation TextRank Results\")\n",
    "evaluation(candidates=candidates_text, references=pp_conc['pp_reference'], sources=pp_conc['pp_source'], algorithm=\"text\", section=\"conc\")\n",
    "print(\"\\nEvaluation LexRank Results\")\n",
    "evaluation(candidates=candidates_lex, references=pp_conc['pp_reference'], sources=pp_conc['pp_source'], algorithm=\"lex\", section=\"conc\")\n",
    "print(\"\\nEvaluation Sumbasic Results\")\n",
    "evaluation(candidates=candidates_sumbasic, references=pp_conc['pp_reference'], sources=pp_conc['pp_source'], algorithm=\"sumbasic\", section=\"conc\")\n",
    "print(\"\\nEvaluation Sumbasic Results\")\n",
    "evaluation(candidates=candidates_bertbasic, references=pp_conc['pp_reference'], sources=pp_conc['pp_source'], algorithm=\"bertbasic\", section=\"conc\")\n",
    "print(\"\\nEvaluation SciBERT Summ Results\")\n",
    "evaluation(candidates=candidates_custombert, references=pp_conc['pp_reference'], sources=pp_conc['pp_source'], algorithm=\"custombert\", section=\"conc\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Evaluation TextRank Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "\n",
      "Evaluation LexRank Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "\n",
      "Evaluation Sumbasic Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# All text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LexRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_lex = summarization_all_files(pp_all, model='lex', section='all', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TextRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_text = summarization_all_files(pp_all, model='texrank', section='all', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SumBasic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "candidates_sumbasic = summarization_all_files( pp_all, model='sumbasic', section='all', SENTENCES_COUNT=3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERT Basic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_bertbasic = summarization_all_files_bert(pp_all, bertbasic, model_name='bert_basic', section='all', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SciBERT Summ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_custombert = summarization_all_files_bert(pp_all, custom_model, model_name='custom_bert', section='all', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "print(\"Evaluation TextRank Results\")\n",
    "evaluation(candidates=candidates_text, references=pp_all['pp_reference'], sources=pp_all['pp_source'], algorithm=\"text\", section=\"all\")\n",
    "print(\"\\nEvaluation LexRank Results\")\n",
    "evaluation(candidates=candidates_lex, references=pp_all['pp_reference'], sources=pp_all['pp_source'], algorithm=\"lex\", section=\"all\")\n",
    "print(\"\\nEvaluation Sumbasic Results\")\n",
    "evaluation(candidates=candidates_sumbasic, references=pp_all['pp_reference'], sources=pp_all['pp_source'], algorithm=\"sumbasic\", section=\"all\")\n",
    "print(\"\\nEvaluation Sumbasic Results\")\n",
    "evaluation(candidates=candidates_bertbasic, references=pp_all['pp_reference'], sources=pp_all['pp_source'], algorithm=\"bertbasic\", section=\"all\")\n",
    "print(\"\\nEvaluation SciBERT Summ Results\")\n",
    "evaluation(candidates=candidates_custombert, references=pp_all['pp_reference'], sources=pp_all['pp_source'], algorithm=\"custombert\", section=\"all\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Evaluation TextRank Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "\n",
      "Evaluation LexRank Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "\n",
      "Evaluation Sumbasic Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}