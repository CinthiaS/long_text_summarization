{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('estudos': conda)"
  },
  "interpreter": {
   "hash": "8189520348208f2d7a55cd5e08e528a8ea0eb6334cdd1bc6eb49a516298cd84a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import *\n",
    "from summarizer import Summarizer\n",
    "from summarizer.coreference_handler import CoreferenceHandler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/media/cinthia/Dados/Mestrado/text-summarizer-long-documents/src')\n",
    "\n",
    "import importlib\n",
    "importlib.reload(evaluate)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4141b4e65593>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sumy.summarizers.lex_rank import LexRankSummarizer as SummarizerLex\n",
    "from sumy.summarizers.sum_basic import SumBasicSummarizer as SummarizerSumBasic\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer  as SummarizerTextrank\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "\n",
    "def summarization_one_file(summarizer, parser, SENTENCES_COUNT):\n",
    "\n",
    "    sentences = []\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        sentences.append(str(sentence))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def summarization_all_files(df, model='lex', section='intro', SENTENCES_COUNT=3):\n",
    "\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    if model == 'lex':\n",
    "\n",
    "        summarizer = SummarizerLex(stemmer)\n",
    "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    \n",
    "    elif model == 'textrank':\n",
    "\n",
    "        summarizer = SummarizerTextrank(stemmer)\n",
    "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "    elif model == 'sumbasic':\n",
    "\n",
    "        summarizer = SummarizerSumBasic(stemmer)\n",
    "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "\n",
    "    summaries = []\n",
    "\n",
    "    f = open(\"{}_{}_summ.txt\".format(model, section), 'w')\n",
    "    for text in df['pp_reference']:\n",
    "\n",
    "        parser = PlaintextParser(text, Tokenizer(LANGUAGE))\n",
    "        summ = summarization_one_file(summarizer, parser, SENTENCES_COUNT=3)\n",
    "        summ = ' '.join(summ)\n",
    "        summaries.append(summ)\n",
    "        f.write(summ)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return summaries"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def summarization_all_files_bert(df, model, model_name='bert_basic', section='intro', SENTENCES_COUNT=3):\n",
    "\n",
    "    summaries = []\n",
    "\n",
    "    f = open(\"{}_{}_summ.txt\".format(model_name, section), 'w')\n",
    "    for text in df['pp_reference']:\n",
    "\n",
    "        summ = model(text, num_sentences=SENTENCES_COUNT)\n",
    "        summaries.append(summ)\n",
    "        f.write(summ)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return summaries"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def load_name_files(path_base, files):\n",
    "\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        texts.append(json.load(open('{}/{}'.format(path_base, file))))\n",
    "\n",
    "    return texts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def load_files(files, path_base):\n",
    "\n",
    "    section_1 = []\n",
    "    section_2 = []\n",
    "    section_3 = []\n",
    "    section_4 = []\n",
    "    keywords = []\n",
    "\n",
    "    texts = load_name_files(path_base, files)\n",
    "\n",
    "    for i in texts:\n",
    "\n",
    "        section_1.append(format_intro(i.get('sec_abstract')))\n",
    "        section_2.append(format_intro(i.get('sec_introduction')))\n",
    "        section_3.append(format_intro(i.get('sec_materials_and_methods')))\n",
    "        section_4.append(format_intro(i.get('sec_results_and_conclusion')))\n",
    "        keywords.append(i.get('sec_keyword'))\n",
    "\n",
    "    return section_1, section_2, section_3, section_4, keywords"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def get_number_sentences(text):\n",
    "\n",
    "    model = Summarizer()\n",
    "    k = model.calculate_optimal_k(text, k_max=5)\n",
    "\n",
    "    return k"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def get_citations(text):\n",
    "\n",
    "  soup = BeautifulSoup(text, 'html.parser')\n",
    "  bib = soup.findAll('xref')\n",
    "\n",
    "  return bib\n",
    "\n",
    "def replace_bib(text, bibs):\n",
    "\n",
    "  for i in bibs:\n",
    "    text = text.replace(str(i), '')\n",
    "    \n",
    "  return text\n",
    "\n",
    "def remove_citations(xml, text):\n",
    "  \n",
    "  bibs = get_citations(xml)\n",
    "  text = replace_bib(text, bibs)\n",
    "  text = format_text(text, post_processing=True)\n",
    "  \n",
    "  return text\n",
    "\n",
    "def format_intro(text):\n",
    "\n",
    "  text = text.replace(\"INTRODUCTION\", \"\")\n",
    "  text = text.replace(\"Introduction\", \"\")\n",
    "  text = text.replace('\\n\\nOBJECTIVE\\n', '')\n",
    "  text = text.replace('\\n\\nObjectives\\n', '')\n",
    "  text = text.replace('\\nSummary\\n\\n', '')\n",
    "  text = text.replace(\"\\n\", \"\")\n",
    "\n",
    "  return text\n",
    "\n",
    "def format_xml(xml):\n",
    "\n",
    "  xml = xml.replace(\".<xref\", \". <xref\")\n",
    "  xml = xml.replace(\"</p>\",\"</p>  \" )\n",
    "  xml = xml.replace('.</p>', \"</p>.\")\n",
    "  xml = xml.replace('<title-introduction><title></title>', '')\n",
    "  xml = xml.replace('</title-introduction>', '')\n",
    "  xml = xml.replace(\"<italic>et al</italic>.\", \"<italic>et al</italic>\")\n",
    "\n",
    "  return xml\n",
    "\n",
    "def format_text(text, post_processing=False):\n",
    "\n",
    "  text = text.replace(\".<xref\", \". <xref\")\n",
    "  text = text.replace(\"</p>\",\"</p> \")\n",
    "  text = text.replace('.</p>', \"</p>.\")\n",
    "  if post_processing:\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    text = text.replace(\"â€“\", '')\n",
    "    text = re.sub(r'(?s)\\(.*?\\)', '', text) \n",
    "    text = re.sub(r'(?s)\\[.*?\\]', '', text) \n",
    "    text = text.replace(\"(,)\", \"\")\n",
    "    text = text.replace(\"()\", \"\")\n",
    "    text = text.replace(\"[,]\", \"\")\n",
    "    text = text.replace(\"[]\", \"\")\n",
    "    text = text.replace(\"(; )\", \"\")\n",
    "    text = text.replace(\"(; )\", \"\")\n",
    "    text = re.sub(r'(?s)<title>.*?</title>', '', text) \n",
    "\n",
    "  return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def preprocess(section, reference):\n",
    "\n",
    "    xml = format_xml(str(section))\n",
    "    text = format_text(str(section), post_processing=False)\n",
    "    reference = format_text(str(reference), post_processing=True)\n",
    "\n",
    "    bibs = get_citations(xml)\n",
    "    text = replace_bib(text, bibs)\n",
    "    text = format_text(text, post_processing=True)\n",
    "\n",
    "    soup = BeautifulSoup(text)\n",
    "    text = soup.get_text()\n",
    "\n",
    "    soup = BeautifulSoup(reference)\n",
    "    reference = soup.get_text()\n",
    "\n",
    "    return text, reference"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def preprocess_all(sources, references):\n",
    "\n",
    "    pp_source = []\n",
    "    pp_references = []\n",
    "\n",
    "    for i in range(len(references)):\n",
    "\n",
    "        text, reference = preprocess(sources[i], references[i])\n",
    "\n",
    "        pp_source.append(text)\n",
    "        pp_references.append(text)\n",
    "\n",
    "    pp_texts = {'pp_source': pp_source, 'pp_reference': pp_references}\n",
    "\n",
    "    return pp_texts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def evaluation(candidates, references, sources, algorithm, section):\n",
    "\n",
    "    metrics=[\"ROUGE_1\", \"ROUGE_2\", \"ROUGE_L\", \"BLEU\"]\n",
    "    evaluate.create_report_valid(\n",
    "            candidates, references, sources,\n",
    "            name_file=\"../validation/validation_{}_{}.xml\".format(algorithm, section),\n",
    "            metrics=metrics)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "def count_len(text):\n",
    "\n",
    "    count_sentences = []\n",
    "    count_words = []\n",
    "\n",
    "    for i in text:\n",
    "        count_sentences.append(len(i.split('.')))\n",
    "        count_words.append(len(i.split(' ')))\n",
    "\n",
    "    print(\"NÃºmero mÃ©dio de sentenÃ§as: {}\".format(np.mean(count_sentences)))\n",
    "    print(\"NÃºmero mÃ©dio de palavras: {}\".format(np.mean(count_words)))\n",
    "\n",
    "    return count_sentences, count_words"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "df = pd.read_csv(\"files.csv\")\n",
    "name_files = df['files'].tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "path_base = '../../sumdata/dataset_articles'\n",
    "section_1, section_2, section_3, section_4, keywords = load_files(name_files, path_base)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "all_section = [section_2[i] + \" \" +  section_3[i] + \" \" + section_4[i] for i in range(len(section_1))]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "LANGUAGE = \"english\"\n",
    "\n",
    "k2 = [3] *len(section_1)\n",
    "k3 = [3] *len(section_1)\n",
    "k4 = [3] *len(section_1)\n",
    "k5 = [9] *len(section_1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "pp_intro = preprocess_all(section_2, section_1)\n",
    "pp_mat = preprocess_all(section_3, section_1)\n",
    "pp_conc = preprocess_all(section_4, section_1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "pp_all = preprocess_all(all_section, section_1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "custom_config = AutoConfig.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "custom_config.output_hidden_states=True\n",
    "custom_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "custom_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', config=custom_config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bertbasic = Summarizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "count_sentences, count_words = count_len(pp_intro['pp_reference'])\n",
    "values['sentences_ref'] = count_sentences\n",
    "values['words_ref'] = count_words"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NÃºmero mÃ©dio de sentenÃ§as: 10.873\n",
      "NÃºmero mÃ©dio de palavras: 210.2152\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "count_sentences, count_words = count_len(pp_intro['pp_source'])\n",
    "values['sentences_intro'] = count_sentences\n",
    "values['words_intro'] = count_words"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NÃºmero mÃ©dio de sentenÃ§as: 22.6116\n",
      "NÃºmero mÃ©dio de palavras: 540.2956\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LexRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "candidates_lex = summarization_all_files(pp_intro, model='lex', section='intro', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TextRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_text = summarization_all_files(pp_intro, model='texrank', section='intro', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SumBasic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_sumbasic = summarization_all_files(pp_intro, model='sumbasic', section='intro', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BERT-Basic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_bertbasic = summarization_all_files_bert(pp_intro, bertbasic, model_name='bert_basic', section='intro', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SciBERT Summ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "candidates_custombert = summarization_all_files_bert(pp_intro, custom_model, model_name='custom_bert', section='intro', SENTENCES_COUNT=3)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-eb1b9ec2fb2c>, line 1)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-eb1b9ec2fb2c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    SciBERT Summ\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "print(\"Evaluation TextRank Results\")\n",
    "evaluation(candidates=candidates_text, references=pp_intro['pp_reference'], sources=pp_intro['pp_source'], algorithm=\"text\", section=\"intro\")\n",
    "print(\"\\nEvaluation LexRank Results\")\n",
    "evaluation(candidates=candidates_lex, references=pp_intro['pp_reference'], sources=pp_intro['pp_source'], algorithm=\"lex\", section=\"intro\")\n",
    "print(\"\\nEvaluation Sumbasic Results\")\n",
    "evaluation(candidates=candidates_sumbasic, references=pp_intro['pp_reference'], sources=pp_intro['pp_source'], algorithm=\"sumbasic\", section=\"intro\")\n",
    "print(\"\\nEvaluation Sumbasic Results\")\n",
    "evaluation(candidates=candidates_bertbasic, references=pp_intro['pp_reference'], sources=pp_intro['pp_source'], algorithm=\"bertbasic\", section=\"intro\")\n",
    "print(\"\\nEvaluation SciBERT Summ Results\")\n",
    "evaluation(candidates=candidates_custombert, references=pp_intro['pp_reference'], sources=pp_intro['pp_source'], algorithm=\"custombert\", section=\"intro\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Materials and Methods"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "count_sentences, count_words = count_len(source)\n",
    "values['sentences_mat'] = count_sentences\n",
    "values['words_mat'] = count_words"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NÃºmero mÃ©dio de sentenÃ§as: 59.1334\n",
      "NÃºmero mÃ©dio de palavras: 1077.3682\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LexRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "candidates_lex = summarization_all_files(pp_mat, model='lex', section='mat', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TextRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_text = summarization_all_files(pp_mat, model='texrank', section='mat', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sumbasic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_sumbasic = summarization_all_files(pp_mat, model='sumbasic', section='mat', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BERT Basic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_bertbasic = summarization_all_files_bert(pp_mat, bertbasic, model_name='bert_basic', section='mat', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scibert summ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_custombert = summarization_all_files_bert(pp_mat, custom_model, model_name='custom_bert', section='mat', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "print(\"Evaluation TextRank Results\")\n",
    "evaluation(candidates=candidates_text, references=pp_mat['pp_reference'], sources=pp_mat['pp_source'], algorithm=\"text\", section=\"mat\")\n",
    "print(\"\\nEvaluation LexRank Results\")\n",
    "evaluation(candidates=candidates_lex, references=pp_mat['pp_reference'], sources=pp_mat['pp_source'], algorithm=\"lex\", section=\"mat\")\n",
    "print(\"\\nEvaluation Sumbasic Results\")\n",
    "evaluation(candidates=candidates_sumbasic, references=pp_mat['pp_reference'], sources=pp_mat['pp_source'], algorithm=\"sumbasic\", section=\"mat\")\n",
    "print(\"\\nEvaluation Sumbasic Results\")\n",
    "evaluation(candidates=candidates_bertbasic, references=pp_mat['pp_reference'], sources=pp_mat['pp_source'], algorithm=\"bertbasic\", section=\"mat\")\n",
    "print(\"\\nEvaluation SciBERT Summ Results\")\n",
    "evaluation(candidates=candidates_custombert, references=pp_mat['pp_reference'], sources=pp_mat['pp_source'], algorithm=\"custombert\", section=\"mat\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Evaluation TextRank Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "\n",
      "Evaluation LexRank Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "\n",
      "Evaluation Sumbasic Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "count_sentences, count_words = count_len(source)\n",
    "values['sentences_conc'] = count_sentences\n",
    "values['words_conc'] = count_words"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NÃºmero mÃ©dio de sentenÃ§as: 109.9564\n",
      "NÃºmero mÃ©dio de palavras: 2081.2918\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LexRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "candidates_lex = summarization_all_files(pp_conc, model='lex', section='conc', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TextRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_text = summarization_all_files(pp_conc, model='texrank', section='conc', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SumBasic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_sumbasic = summarization_all_files(pp_conc, model='sumbasic', section='conc', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BERT - Basic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_bertbasic = summarization_all_files_bert(pp_conc, bertbasic, model_name='bert_basic', section='conc', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scibert Summ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_custombert = summarization_all_files_bert(pp_conc, custom_model, model_name='custom_bert', section='conc', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "print(\"Evaluation TextRank Results\")\n",
    "evaluation(candidates=candidates_text, references=pp_conc['pp_reference'], sources=pp_conc['pp_source'], algorithm=\"text\", section=\"conc\")\n",
    "print(\"\\nEvaluation LexRank Results\")\n",
    "evaluation(candidates=candidates_lex, references=pp_conc['pp_reference'], sources=pp_conc['pp_source'], algorithm=\"lex\", section=\"conc\")\n",
    "print(\"\\nEvaluation Sumbasic Results\")\n",
    "evaluation(candidates=candidates_sumbasic, references=pp_conc['pp_reference'], sources=pp_conc['pp_source'], algorithm=\"sumbasic\", section=\"conc\")\n",
    "print(\"\\nEvaluation Sumbasic Results\")\n",
    "evaluation(candidates=candidates_bertbasic, references=pp_conc['pp_reference'], sources=pp_conc['pp_source'], algorithm=\"bertbasic\", section=\"conc\")\n",
    "print(\"\\nEvaluation SciBERT Summ Results\")\n",
    "evaluation(candidates=candidates_custombert, references=pp_conc['pp_reference'], sources=pp_conc['pp_source'], algorithm=\"custombert\", section=\"conc\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Evaluation TextRank Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "\n",
      "Evaluation LexRank Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "\n",
      "Evaluation Sumbasic Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# All text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LexRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_lex = summarization_all_files(pp_all, model='lex', section='all', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TextRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_text = summarization_all_files(pp_all, model='texrank', section='all', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SumBasic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "candidates_sumbasic = summarization_all_files( pp_all, model='sumbasic', section='all', SENTENCES_COUNT=3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BERT Basic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_bertbasic = summarization_all_files_bert(pp_all, bertbasic, model_name='bert_basic', section='all', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SciBERT Summ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "candidates_custombert = summarization_all_files_bert(pp_all, custom_model, model_name='custom_bert', section='all', SENTENCES_COUNT=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "print(\"Evaluation TextRank Results\")\n",
    "evaluation(candidates=candidates_text, references=pp_all['pp_reference'], sources=pp_all['pp_source'], algorithm=\"text\", section=\"all\")\n",
    "print(\"\\nEvaluation LexRank Results\")\n",
    "evaluation(candidates=candidates_lex, references=pp_all['pp_reference'], sources=pp_all['pp_source'], algorithm=\"lex\", section=\"all\")\n",
    "print(\"\\nEvaluation Sumbasic Results\")\n",
    "evaluation(candidates=candidates_sumbasic, references=pp_all['pp_reference'], sources=pp_all['pp_source'], algorithm=\"sumbasic\", section=\"all\")\n",
    "print(\"\\nEvaluation Sumbasic Results\")\n",
    "evaluation(candidates=candidates_bertbasic, references=pp_all['pp_reference'], sources=pp_all['pp_source'], algorithm=\"bertbasic\", section=\"all\")\n",
    "print(\"\\nEvaluation SciBERT Summ Results\")\n",
    "evaluation(candidates=candidates_custombert, references=pp_all['pp_reference'], sources=pp_all['pp_source'], algorithm=\"custombert\", section=\"all\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Evaluation TextRank Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "\n",
      "Evaluation LexRank Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "\n",
      "Evaluation Sumbasic Results\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}